## Home_Assignment_5
## Student Name : Sai krishna Edara
## Student ID : 700769262
### Question 3
### Basic Generative Adversarial Network (GAN) for MNIST

This project implements a basic Generative Adversarial Network (GAN) to generate handwritten digit images from the MNIST dataset. It uses PyTorch for building and training the GAN.

## Overview

The GAN consists of two main neural networks:

* **Generator (G):** Takes random noise as input and tries to generate realistic-looking images.
* **Discriminator (D):** Takes an image (either real from the MNIST dataset or fake generated by G) and tries to distinguish between real and fake images.

The generator and discriminator are trained in an adversarial manner. The generator tries to fool the discriminator by producing increasingly realistic images, while the discriminator tries to become better at distinguishing real from fake.

## Setup

### Prerequisites

* Python 3.x
* PyTorch (>= 1.0)
* Torchvision
* Matplotlib
* NumPy
* `os` module (built-in)

### Installation

1.  **Clone the repository** (if you have it in one) or simply save the provided Python script (`.py` file).
2.  **Install the required libraries:**
    ```bash
    pip install torch torchvision matplotlib numpy
    ```

## Usage

1.  **Save the provided Python code** into a file named (for example) `gan_mnist.py`.
2.  **Run the script** from your terminal:
    ```bash
    python gan_mnist.py
    ```

### Configuration

The script includes the following hyperparameters that you can adjust:

* `latent_dim`: The dimension of the random noise vector input to the generator (default: 100).
* `batch_size`: The number of images processed in each training iteration (default: 128).
* `lr`: The learning rate for both the generator and the discriminator optimizers (default: 0.0002).
* `epochs`: The total number of training epochs (default: 100).
* `image_dir`: The directory where generated images will be saved (default: "gan\_images").

You can modify these parameters directly in the script.

### Output

During training, the script will:

* Print the generator and discriminator losses for each epoch.
* Save sample generated images to the `gan_images` directory at epochs 0, 50, and 100. The filenames will be `epoch_0.png`, `epoch_50.png`, and `epoch_100.png`.
* Display the generated sample images at the specified epochs during the training process.
* After training is complete, it will generate a plot of the generator and discriminator losses over time and save it as `gan_losses.png`. It will also display this loss plot.

## Code Explanation

### Modules

* `torch`: Core PyTorch library for tensor operations and neural networks.
* `torch.nn`: Contains modules for building neural network layers.
* `torch.optim`: Provides various optimization algorithms.
* `torchvision.datasets`: Includes popular datasets like MNIST.
* `torchvision.transforms`: Offers common image transformations.
* `torch.utils.data.DataLoader`: Utility for loading data in batches.
* `torchvision.utils.make_grid`, `torchvision.utils.save_image`: Functions for creating and saving a grid of images.
* `matplotlib.pyplot`: Used for plotting the loss curves and displaying images.
* `os`: For creating directories.
* `numpy`: For numerical operations, especially when displaying images.

### Models

* **Generator:** A sequential neural network with linear layers and ReLU activations, followed by a final linear layer with a Tanh activation to output images in the range [-1, 1]. The output is reshaped to the MNIST image dimensions (1x28x28).
* **Discriminator:** A sequential neural network that flattens the input image and then uses linear layers with LeakyReLU activations, followed by a final linear layer with a Sigmoid activation to output a probability (between 0 and 1) indicating whether the input image is real or fake.

### Training Loop

The training loop iterates through the specified number of epochs. In each epoch, it processes batches of real MNIST images and generates fake images.

1.  **Discriminator Training:** The discriminator is trained to distinguish between real and fake images. The loss is calculated as the sum of the binary cross-entropy loss for real images (labeled as 1) and fake images (labeled as 0).
2.  **Generator Training:** The generator is trained to fool the discriminator. It generates fake images, and the loss is calculated based on how well these fake images are classified as real by the discriminator. The generator aims to maximize this loss.

The optimizers (`Adam`) are used to update the weights of the generator and discriminator based on their respective loss gradients.

### Image Saving and Loss Plotting

The `save_and_show_sample` function generates a batch of fake images using the fixed noise and saves them as a grid. The loss values for both the generator and discriminator are stored during training and then plotted at the end to visualize the training progress.

## Further Improvements

This is a basic GAN implementation. Potential improvements include:

* Using deeper and more complex network architectures for the generator and discriminator (e.g., convolutional layers).
* Implementing techniques to stabilize training, such as batch normalization or weight normalization.
* Exploring different loss functions (e.g., Wasserstein loss).
* Experimenting with different optimizers and learning rate schedules.
* Using techniques for conditional image generation (e.g., Conditional GANs - CGANs) to generate specific digits.

  

# Question 4
# Data Poisoning Simulation on a Sentiment Classifier

This project simulates a data poisoning attack on a basic sentiment classifier trained on a small, synthetic movie review dataset. The attack involves flipping the sentiment labels of reviews containing the phrase "UC Berkeley" in the training data.

## Overview

The script performs the following steps:

1.  **Creates a small, synthetic dataset** of movie reviews with positive and negative sentiment labels.
2.  **Splits the dataset** into training and testing sets.
3.  **Extracts features** from the text reviews using the TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer.
4.  **Trains a baseline Logistic Regression classifier** on the original, clean training data.
5.  **Simulates a data poisoning attack** by iterating through the training data and flipping the sentiment label for any review containing the phrase "UC Berkeley".
6.  **Trains a new Logistic Regression classifier** on the poisoned training data.
7.  **Evaluates and visualizes** the performance of both the baseline and the poisoned models using accuracy scores and confusion matrices. It also compares their accuracies using a bar chart.

## Setup

### Prerequisites

* Python 3.x
* NumPy
* Pandas
* scikit-learn (sklearn)
* Matplotlib
* Seaborn

### Installation

You can install the necessary libraries using pip:

```bash
pip install numpy pandas scikit-learn matplotlib seaborn
```

### Usage
Save the following Python code into a file named (for example) poisoning_simulation.py.

Run the script from your terminal:
```
python poisoning_simulation.py
```
### Output
The script will print the following to the console:

The accuracy and confusion matrix of the baseline sentiment classifier.
The accuracy and confusion matrix of the sentiment classifier trained on the poisoned data.
Additionally, it will generate two plots:

**Confusion Matrices:** Two heatmaps displayed side-by-side, showing the confusion matrix for the baseline model (in Blues colormap) and the poisoned model (in Reds colormap). The axes are labeled "Predicted" and "Actual" sentiment (Negative and Positive).
**Accuracy Comparison:** A bar chart comparing the accuracy of the baseline model and the model trained on the poisoned data.

## Code Explanation
#### Libraries
numpy: For numerical operations.
pandas: For creating and manipulating DataFrames.
sklearn.model_selection.train_test_split: For splitting the data into training and testing sets.
sklearn.feature_extraction.text.TfidfVectorizer: For converting text data into numerical feature vectors.
sklearn.linear_model.LogisticRegression: The classification model used.
sklearn.metrics.accuracy_score: For calculating the accuracy of the model.
sklearn.metrics.confusion_matrix: For generating the confusion matrix.
matplotlib.pyplot: For creating plots.
seaborn: For creating informative and visually appealing statistical graphics, used here for the heatmaps and bar chart.

#### Training and Evaluation
Two separate Logistic Regression models are trained: one on the original data (baseline) and one on the poisoned data. Both models are then evaluated on the same test set to ensure a fair comparison of the impact of the poisoning attack.

#### Visualization
The matplotlib and seaborn libraries are used to visualize the confusion matrices and the accuracy comparison, providing a clear visual representation of how the data poisoning affected the model's performance.

#### Observations
Running this script will likely show:

A decrease in the accuracy of the model trained on the poisoned data compared to the baseline model.
Changes in the confusion matrix, indicating that the poisoned model makes different types of errors, particularly potentially misclassifying sentiments related to the poisoned entity.
